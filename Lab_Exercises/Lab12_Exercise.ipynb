{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Lab05 Exercise: Object Segmentation\n",
    "\n",
    "In this lab, the goal is to write a program to segment different objects using the **GMM and EM** algorithm. We also use <u>*k-means* clustering algorithm to initialize the parameters</u> of GMM. The following steps should be implemented to achieve such a goal:\n",
    "\n",
    "1. Load image\n",
    "2. Initialize parameters of GMM using K-means\n",
    "3. Implement the EM algorithm for GMM\n",
    "4. Display result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.cluster import KMeans\n",
    "import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "COLORS = [\n",
    "    (255, 0, 0),   # red\n",
    "    (0, 255, 0),  # green\n",
    "    (0, 0, 255),   # blue\n",
    "    (255, 255, 0), # yellow\n",
    "    (255, 0, 255), # magenta\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Image\n",
    "What you should do is to implement Z-score normalization in `load()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "def loadImage(image_path):\n",
    "    image = np.array(cv2.imread(image_path))\n",
    "    h, w, c = image.shape\n",
    "    image = image.reshape((h * w, c))\n",
    "\n",
    "    # TODO: please normalize image_pixl using Z-score\n",
    "    _mean = np.mean(image, axis=0)\n",
    "    _std = np.std(image, axis=0)\n",
    "    image_norm = (image - _mean) / _std\n",
    "\n",
    "    return h, w, c, image_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize means, covariance matrices and mixing coefficients of GMM\n",
    "k-means is used to initialize means, covariance matrices and mixing coefficients of GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(n_cluster, image_pixl):\n",
    "    kmeans = KMeans(n_clusters=n_cluster)# instantiate a K-means\n",
    "    labels = kmeans.fit_predict(image_pixl)# fit and get clustering result\n",
    "    initial_mus = kmeans.cluster_centers_# get centroids\n",
    "    initial_priors, initial_covs = [], []\n",
    "    #Followings are for initialization:\n",
    "    for i in range(n_cluster):\n",
    "        datas = image_pixl[labels == i, ...].T\n",
    "        initial_covs.append(np.cov(datas))\n",
    "        initial_priors.append(datas.shape[1] / len(labels))\n",
    "    return initial_mus, initial_priors, initial_covs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement GMM algorithm\n",
    "We use EM algorithm to refine GMM's parameters.\n",
    "\n",
    "Although it may be not easy for some students to derive EM formula for GMM, GMM isn't very difficult to implement once you have the formula. Therefore, to help you understand GMM more, there are still some blanks for you to fill in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM:\n",
    "    def __init__(self, ncomp, initial_mus, initial_covs, initial_priors):\n",
    "        \"\"\"\n",
    "        :param ncomp:           the number of clusters\n",
    "        :param initial_mus:     initial means\n",
    "        :param initial_covs:    initial covariance matrices\n",
    "        :param initial_priors:  initial mixing coefficients\n",
    "        \"\"\"\n",
    "        self.ncomp = ncomp\n",
    "        self.mus = np.asarray(initial_mus)\n",
    "        self.covs = np.asarray(initial_covs)\n",
    "        self.priors = np.asarray(initial_priors)\n",
    "\n",
    "    def inference(self, datas):\n",
    "        \"\"\"\n",
    "        E-step\n",
    "        :param datas:   original data\n",
    "        :return:        posterior probability (gamma) and log likelihood\n",
    "        \"\"\"\n",
    "        probs = []\n",
    "        for i in range(self.ncomp):\n",
    "            mu, cov, prior = self.mus[i, :], self.covs[i, :, :], self.priors[i]\n",
    "            prob = prior * multivariate_normal.pdf(\n",
    "                datas, mean=mu, cov=cov, allow_singular=True\n",
    "            )\n",
    "            probs.append(np.expand_dims(prob, -1))\n",
    "        preds = np.concatenate(probs, axis=1)\n",
    "\n",
    "        # TODO: calc log likelihood\n",
    "        log_likelihood = np.sum(np.log(np.sum(preds, axis=1)))\n",
    "\n",
    "        # TODO: calc gamma\n",
    "        gamma = np.ndarray((datas.shape[0], self.ncomp))\n",
    "        summ = np.sum(preds, axis=1)\n",
    "        for i in range(0, datas.shape[0]):\n",
    "            for j in range(0, self.ncomp):\n",
    "                gamma[i, j] = (self.priors[j] * preds[i, j]) / summ[i]\n",
    "\n",
    "        return gamma, log_likelihood\n",
    "\n",
    "    def update(self, datas:np.ndarray, gamma):\n",
    "            \"\"\"\n",
    "            M-step\n",
    "            :param datas:   original data\n",
    "            :param gamma:    gamma\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            new_mus, new_covs, new_priors = [], [], []\n",
    "            labels = np.argmax(gamma, axis=1)\n",
    "            soft_counts = np.sum(gamma, axis=0)\n",
    "            \n",
    "            (N, dimension) = datas.shape\n",
    "            \n",
    "            \n",
    "            for i in range(self.ncomp):\n",
    "                # TODO: calc mu\n",
    "                X_ks:np.ndarray = datas[labels == i]\n",
    "                N_i = X_ks.shape[0]\n",
    "                if(N_i == 0):\n",
    "                    continue\n",
    "                \n",
    "                new_mu = np.zeros((dimension, ))\n",
    "                for n in range(N):\n",
    "                    new_mu += gamma[n, i]*datas[n]\n",
    "                    \n",
    "                \n",
    "                new_mu /= N_i\n",
    "                    \n",
    "                new_mus.append(new_mu)\n",
    "                # TODO: calc cov\n",
    "                new_cov = np.ndarray((dimension, dimension))\n",
    "                for n in range(N):\n",
    "                    new_cov += gamma[n, i]*((datas[n] - new_mus[i]) * (datas[n] - new_mus[i]).T)\n",
    "                \n",
    "                \n",
    "                new_covs.append(new_cov)\n",
    "\n",
    "                # TODO: calc mixing coefficients\n",
    "                new_prior = N_i/N\n",
    "                new_priors.append(new_prior)\n",
    "                \n",
    "\n",
    "            self.mus = np.asarray(new_mus)\n",
    "            self.covs = np.asarray(new_covs)\n",
    "            self.priors = np.asarray(new_priors)\n",
    "\n",
    "    def fit(self, data, iteration):\n",
    "        prev_log_liklihood = None\n",
    "\n",
    "        bar = tqdm.tqdm(total=iteration)\n",
    "        for i in range(iteration):\n",
    "            gamma, log_likelihood = self.inference(data)\n",
    "            self.update(data, gamma)\n",
    "            if (\n",
    "                prev_log_liklihood is not None\n",
    "                and abs(log_likelihood - prev_log_liklihood) < 1e-10\n",
    "            ):\n",
    "                break\n",
    "            prev_log_likelihood = log_likelihood\n",
    "\n",
    "            bar.update()\n",
    "            bar.set_postfix({\"log likelihood\": log_likelihood})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display\n",
    "We use `matplotlib` to display what we segment, you can check the code in `visualize()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualize(gmm, image, ncomp, ih, iw):\n",
    "    beliefs, log_likelihood = gmm.inference(image)\n",
    "    map_beliefs = np.reshape(beliefs, (ih, iw, ncomp))\n",
    "    segmented_map = np.zeros((ih, iw, 3))\n",
    "    for i in range(ih):\n",
    "        for j in range(iw):\n",
    "            hard_belief = np.argmax(map_beliefs[i, j, :])\n",
    "            segmented_map[i, j, :] = np.asarray(COLORS[hard_belief]) / 255.0\n",
    "    plt.imshow(segmented_map)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\CS329_Machine_Learning\\CS329_Machine_Learning\\Lab_Exercises\\Lab12_Exercise.ipynb 单元格 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/CS329_Machine_Learning/CS329_Machine_Learning/Lab_Exercises/Lab12_Exercise.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ih, iw, ic, image_norm \u001b[39m=\u001b[39m load(\u001b[39m\"\u001b[39m\u001b[39mdata/original/sample.png\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/CS329_Machine_Learning/CS329_Machine_Learning/Lab_Exercises/Lab12_Exercise.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m ncomp \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/CS329_Machine_Learning/CS329_Machine_Learning/Lab_Exercises/Lab12_Exercise.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m iteration\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load' is not defined"
     ]
    }
   ],
   "source": [
    "ih, iw, ic, image_norm = load(\"data/original/sample.png\")\n",
    "ncomp = 3\n",
    "iteration=500\n",
    "# init mu, prior and cov\n",
    "initial_mus, initial_priors, initial_covs = kmeans(ncomp, image_norm)\n",
    "\n",
    "# GMM\n",
    "print(\"GMM begins...\")\n",
    "gmm = GMM(ncomp, initial_mus, initial_covs, initial_priors)\n",
    "gmm.fit(image_norm, iteration)\n",
    "\n",
    "# visualize\n",
    "visualize(gmm, image_norm, ncomp, ih, iw)\n",
    "print(\"Finish!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample Result\n",
    "<img src=\"images/image-20220804223008133.png\" alt=\"image-20220804223008133\" style=\"zoom:67%;\" />\n",
    "<img src=\"images/image-20220804222915979.png\" alt=\"image-20220804222915979\" style=\"zoom: 67%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions(3 points)\n",
    "1. What are the strengths of GMM; when does it perform well?\n",
    "2. What are the weaknesses of GMM; when does it perform poorly?\n",
    "3. What makes GMM a good candidate for the clustering problem, if you have enough knowledge about the data?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS329",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
