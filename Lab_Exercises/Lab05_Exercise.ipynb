{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Lab05 Exercise: Iris Classification\n",
    "This lab introduces classical machine learning algorithms, decision trees (DTs) and their ensemble learning (e.g., Random Forests). Decision trees are important non-parameter learning methods. Although DTs are simple and limited, they still can achieve excellent performance using ensemble learning schemes.\n",
    "\n",
    "For this lab assignment, we'll use the algorithms we've learned today to fit the model and evaluate the model’s prediction performance. The scikit-learn package will be used to save your time.\n",
    "\n",
    "\n",
    "### Decision tree\n",
    "- <font size=4>Step 1. load iris dataset </font>\n",
    "\n",
    "Datasets: First, we load the scikit-learn iris toy dataset ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font size=4> Step 2. Define the features and the target </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[:,2:]\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font size=4> Step 3. Visualization </font>\n",
    "  \n",
    "    We need to use proper visualization methods to have an intuitive understanding.\n",
    "\n",
    "    For visualization, only the last 2 attributes are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[y==0, 0], X[y==0, 1])\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1])\n",
    "plt.scatter(X[y==2, 0], X[y==2, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font size=4> Step 4. Preprocessing data </font>\n",
    "Please check whether the data needs to be preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font size=4> Step 5. Split the dataset into train and test sets </font>\n",
    "  \n",
    " Now we divide the whole dataset into a training set and a test set using the the scikit-learn model_selection module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font size=4> Step 6. Explore the model parameters </font>\n",
    "\n",
    "  Decision trees are quite easy to use, but they are prone to overfit the training data. Actually almost all the non-parameter learning methods suffer from this problem. We can use pruning to optimize our trained decision trees; we can also adjust the super parameters to avoid overfitting.\n",
    "\n",
    "  The decision tree model  given by the `SkLearn`  is as follows:\n",
    "\n",
    "  ```python\n",
    "  DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)\n",
    "  ```\n",
    "\n",
    "  There are so many arguments and they are all helpful in adjusting the algorithm parameters to achieve the balance between bias and variance.  \n",
    "  Adjust these parameters: `criterion`,`max_depth`, `min_samples_leaf`,  `min_samples_split` , `max_leaf_nodes `,`min_impurity_split `\n",
    "  and explain how it affects the bias and variance of the classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, select the best set of parameters for the following steps.\n",
    "- <font size=4> Step 7. Use the model of your choice on the test set </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font size=4> Step 8. Evaluate the model </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font size=4> Step 9. Visual decision boundary and generated decision tree </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest\n",
    "\n",
    "In this section, you are required to use random forests for classification. Thus, in `scikit-learn`, there are two ways to implement a random forset, from the Bagging view and from the RF view.<br>\n",
    "Classify `iris`  using `BaggingClassifier( )` and `RandomForestClassifier( )` respectively, \n",
    "\n",
    "- <font color=blue >**RF view:**</font> we construct a RF class directly.\n",
    "  \n",
    "```python\n",
    "# Use Random Forest directly\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=300,\n",
    "                                random_state=666, # random attributes subset\n",
    "                                oob_score=True,\n",
    "                                # n_jobs=-1\n",
    "                               )\n",
    "rf_clf.fit(X,y)\n",
    "```\n",
    "\n",
    "- <font color=blue face=雅黑>**Bagging view:**</font>  we use the bagging algorithm with a number of base learning algorithms of decision trees.\n",
    "  \n",
    "```python\n",
    "# Use Random Forest from Bagging view\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bagging_clf = BaggingClassifier(DecisionTreeClassifier(),\n",
    "                                n_estimators=300,\n",
    "                                max_samples=300,\n",
    "                                bootstrap=True, # using bootstrap sampling method\n",
    "                                oob_score=True, # use oob data for scoring\n",
    "                                # n_jobs=-1 # use paralell computing\n",
    "                               )\n",
    "bagging_clf.fit(X,y)\n",
    "```\n",
    "\n",
    "- Compare the performances of two  methods, and select different parameters for model  and evaluate the model using bias and variance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other ensemble learning\n",
    "For classification, we have many models to choose . Please don't just pick a model to train and say it's good enough. We need to select models based on some metrics, such as choosing models with low bias and low variance.\n",
    "\n",
    "In this part, you are required  to use `AdaBoost` and `Gradient boosting`.Compare their performances with decision tree and random forest, and finally select the best model  and the optimal  parameters for iris classification.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font size=4>Hint: About how to select models and parameters:</font>\n",
    "- Select model using cross validation. Compare the scores in the training set and the validation set. If they are good enough, use the model in the test set.\n",
    "- Calculate the bias and variance of each model to further analyze your chosen model.\n",
    "- Select parameters using cross validation\n",
    "  \n",
    "### Questions:\n",
    "(1) Can decision trees and random forests be used for unsupervised clustering or data dimension reduction? Why?\n",
    "\n",
    "(2) What are the strengths of the decision tree/random forest methods; when do they perform well?\n",
    "\n",
    "(3) What are the weaknesses of the decision tree/random forest methods; when do they perform poorly?\n",
    "\n",
    "(4) What makes the decision tree/random forest a good candidate for the classification/regression problem, if you have enough knowledge about the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- Decision trees are prone to overfitting, but random forest algorithm prevents overfitting.\n",
    "- Random forest algorithm is comparatively time-consuming, whereas decision tree algorithm gives fast results.\n",
    "- There are many arguments for either base decision trees or the whole ensemble algorithm.  A good ensemble algorithm should make sure that base ones are both accurate and diversified.  So it is better to get a set of good enough base tree parameters before training the ensemble learning algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "https://scikit-learn.org/stable/"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
