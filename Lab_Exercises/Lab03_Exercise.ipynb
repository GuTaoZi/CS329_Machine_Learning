{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "559e98b8-429e-4293-a354-836ba453125e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Instruction\n",
    "Spam filtering is a beginner’s example of the document classification task which involves classifying an email as spam or non-spam (a.k.a. ham) mail. An email dataset will be provided. We will use the following steps to build this application:\n",
    "1) Preparing the text data\n",
    "2) Creating a word dictionary\n",
    "3) Feature extraction\n",
    "4) Training the classifier\n",
    "5) Checking the results on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f818bbc-ff4e-4e14-9e91-80aa1f1b681e",
   "metadata": {},
   "source": [
    "### Preparing the text data\n",
    "The data-set used here, is split into a training set and a test set containing 702 mails and 260 mails respectively, divided equally between spam and ham mails. You will easily recognize spam mails as it contains `spmsg` in its filename.\n",
    "\n",
    "In any text mining problem, text cleaning is the first step where we remove those words from the document which may not contribute to the information we want to extract. Emails may contain a lot of undesirable characters like punctuation marks, stop words, digits, etc which may not be helpful in detecting the spam email. The emails in Ling-spam corpus have been already preprocessed in the following ways:\n",
    "\n",
    "1. **Removal of stop words** – Stop words like “and”, “the”, “of”, etc are very common in all English sentences and are not very meaningful in deciding spam or legitimate status, so these words have been removed from the emails.\n",
    "\n",
    "2. **Lemmatization** – It is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. For example, “include”, “includes,” and “included” would all be represented as “include”. The context of the sentence is also preserved in lemmatization as opposed to stemming (another buzz word in text mining which does not consider meaning of the sentence)\n",
    "\n",
    "We still need to remove the non-words like punctuation marks or special characters from the mail documents. There are several ways to do it. Here, we will remove such words after creating a dictionary, which is a very convenient method to do so since when you have a dictionary; you need to remove every such word only once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80603548-feb4-4b5d-aea6-f1e87b38f5ea",
   "metadata": {},
   "source": [
    "### Creating word dictionary\n",
    "We will only perform text analytics on the content to detect the spam mails. As the first step, we need to create a dictionary of words and their frequency. For this task, a training set of 700 mails is utilized. This python function will create the dictionary for you.\n",
    "```Python\n",
    "def make_Dictionary(train_dir):\n",
    "    emails = [os.path.join(train_dir,f) for f in os.listdir (train_dir)]\n",
    "    all_words = []\n",
    "    for mail in emails:\n",
    "        with open (mail) as m:\n",
    "            for i,line in enumerate (m) :\n",
    "                if i == 2:\n",
    "                    words = line.split()\n",
    "                    all_words += words\n",
    "    dictionary = Counter(all_words)\n",
    "    # Paste code for non-word removal here\n",
    "    \n",
    "    return dictionary\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c753f2-4b1f-41d8-9e57-554731d42be2",
   "metadata": {},
   "source": [
    "Once the dictionary is created we can add just a few lines of code written below to the above function to remove non-words. Absurd single characters in the dictionary which are irrelevant here are also removed. Do not forget to insert the below code in the function of make_Dictionary:\n",
    "```python\n",
    "list_to_remove = list(dictionary.keys())\n",
    "for item in list_to_remove:\n",
    "    if item.isalpha() == False:\n",
    "        del dictionary[item]\n",
    "    elif len(item) == 1:\n",
    "        del dictionary[item]\n",
    "dictionary = dictionary.most_common(3000)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc41ebff-aad3-4d08-8f6a-4e4ed73f278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "def make_Dictionary(train_dir):\n",
    "    emails = [os.path.join(train_dir,f) for f in os.listdir (train_dir)]\n",
    "    all_words = []\n",
    "    for mail in emails:\n",
    "        with open (mail) as m:\n",
    "            for i,line in enumerate (m) :\n",
    "                if i == 2:\n",
    "                    words = line.split()\n",
    "                    all_words += words\n",
    "    dictionary = Counter(all_words)\n",
    "    list_to_remove = list(dictionary.keys())\n",
    "    for item in list_to_remove:\n",
    "        if item.isalpha() == False:\n",
    "            del dictionary[item]\n",
    "        elif len(item) == 1:\n",
    "            del dictionary[item]\n",
    "    dictionary = dictionary.most_common(3000)\n",
    "    # Paste code for non-word removal here\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32b7058-baff-464b-9a19-4dd6b8605bdc",
   "metadata": {},
   "source": [
    "The dictionary can be seen by the command “print dictionary”. You may find some absurd word counts to be high but don’t worry, it’s just a dictionary and you always have a chance to improve it later. If you use the provided dataset, make sure your dictionary has some of the entries given below as most frequent words. Here 3000 most frequently used words are chosen in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0dc9f0e8-72fb-4d94-9067-5ffbb8e1ca5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('order', 1414),\n",
       " ('address', 1293),\n",
       " ('report', 1216),\n",
       " ('mail', 1127),\n",
       " ('send', 1079),\n",
       " ('language', 1072),\n",
       " ('email', 1051),\n",
       " ('program', 1001),\n",
       " ('our', 987),\n",
       " ('list', 935),\n",
       " ('one', 917),\n",
       " ('name', 878),\n",
       " ('receive', 826),\n",
       " ('money', 788),\n",
       " ('free', 762),\n",
       " ('work', 755),\n",
       " ('information', 677),\n",
       " ('business', 654),\n",
       " ('please', 652),\n",
       " ('university', 595),\n",
       " ('us', 564),\n",
       " ('day', 556),\n",
       " ('follow', 544),\n",
       " ('internet', 520),\n",
       " ('over', 511),\n",
       " ('http', 479),\n",
       " ('check', 472),\n",
       " ('call', 469),\n",
       " ('each', 466),\n",
       " ('include', 452),\n",
       " ('com', 448),\n",
       " ('linguistic', 442),\n",
       " ('number', 423),\n",
       " ('want', 420),\n",
       " ('letter', 419),\n",
       " ('need', 418),\n",
       " ('many', 412),\n",
       " ('here', 397),\n",
       " ('market', 395),\n",
       " ('start', 390),\n",
       " ('even', 386),\n",
       " ('fax', 383),\n",
       " ('form', 380),\n",
       " ('most', 377),\n",
       " ('first', 373),\n",
       " ('web', 366),\n",
       " ('service', 363),\n",
       " ('interest', 362),\n",
       " ('software', 352),\n",
       " ('remove', 349),\n",
       " ('read', 347),\n",
       " ('those', 345),\n",
       " ('week', 344),\n",
       " ('every', 332),\n",
       " ('credit', 329),\n",
       " ('ll', 326),\n",
       " ('site', 320),\n",
       " ('much', 318),\n",
       " ('english', 318),\n",
       " ('edu', 318),\n",
       " ('product', 317),\n",
       " ('bulk', 312),\n",
       " ('phone', 311),\n",
       " ('must', 299),\n",
       " ('two', 298),\n",
       " ('offer', 297),\n",
       " ('cost', 294),\n",
       " ('best', 291),\n",
       " ('www', 290),\n",
       " ('computer', 289),\n",
       " ('link', 283),\n",
       " ('state', 279),\n",
       " ('card', 278),\n",
       " ('de', 272),\n",
       " ('home', 268),\n",
       " ('available', 266),\n",
       " ('system', 264),\n",
       " ('message', 260),\n",
       " ('own', 260),\n",
       " ('after', 258),\n",
       " ('question', 257),\n",
       " ('opportunity', 257),\n",
       " ('place', 256),\n",
       " ('pay', 256),\n",
       " ('within', 254),\n",
       " ('million', 254),\n",
       " ('write', 253),\n",
       " ('hour', 253),\n",
       " ('world', 253),\n",
       " ('sell', 251),\n",
       " ('help', 248),\n",
       " ('copy', 248),\n",
       " ('show', 248),\n",
       " ('below', 247),\n",
       " ('same', 242),\n",
       " ('conference', 241),\n",
       " ('file', 238),\n",
       " ('through', 235),\n",
       " ('directory', 234),\n",
       " ('before', 232),\n",
       " ('where', 229),\n",
       " ('easy', 229),\n",
       " ('book', 226),\n",
       " ('try', 226),\n",
       " ('income', 226),\n",
       " ('thank', 225),\n",
       " ('tell', 222),\n",
       " ('today', 220),\n",
       " ('company', 219),\n",
       " ('member', 216),\n",
       " ('word', 215),\n",
       " ('example', 215),\n",
       " ('back', 214),\n",
       " ('different', 213),\n",
       " ('advertise', 213),\n",
       " ('month', 210),\n",
       " ('subject', 209),\n",
       " ('etc', 207),\n",
       " ('ask', 206),\n",
       " ('contact', 206),\n",
       " ('response', 204),\n",
       " ('cash', 204),\n",
       " ('little', 203),\n",
       " ('learn', 203),\n",
       " ('add', 202),\n",
       " ('cd', 200),\n",
       " ('linguist', 198),\n",
       " ('research', 198),\n",
       " ('per', 198),\n",
       " ('type', 197),\n",
       " ('down', 196),\n",
       " ('process', 196),\n",
       " ('anyone', 196),\n",
       " ('again', 196),\n",
       " ('right', 196),\n",
       " ('least', 195),\n",
       " ('is', 195),\n",
       " ('ve', 195),\n",
       " ('special', 194),\n",
       " ('price', 193),\n",
       " ('level', 192),\n",
       " ('provide', 192),\n",
       " ('save', 192),\n",
       " ('re', 190),\n",
       " ('off', 190),\n",
       " ('live', 190),\n",
       " ('require', 188),\n",
       " ('few', 186),\n",
       " ('change', 186),\n",
       " ('line', 186),\n",
       " ('sure', 186),\n",
       " ('financial', 186),\n",
       " ('box', 185),\n",
       " ('post', 185),\n",
       " ('search', 185),\n",
       " ('let', 184),\n",
       " ('ffa', 183),\n",
       " ('never', 181),\n",
       " ('paper', 179),\n",
       " ('thing', 177),\n",
       " ('four', 177),\n",
       " ('teach', 175),\n",
       " ('case', 175),\n",
       " ('life', 175),\n",
       " ('page', 174),\n",
       " ('participate', 173),\n",
       " ('text', 172),\n",
       " ('great', 172),\n",
       " ('really', 171),\n",
       " ('between', 170),\n",
       " ('both', 170),\n",
       " ('net', 170),\n",
       " ('buy', 169),\n",
       " ('success', 167),\n",
       " ('plan', 167),\n",
       " ('part', 166),\n",
       " ('move', 164),\n",
       " ('request', 164),\n",
       " ('instruction', 164),\n",
       " ('simply', 164),\n",
       " ('click', 164),\n",
       " ('package', 162),\n",
       " ('reply', 161),\n",
       " ('less', 160),\n",
       " ('point', 160),\n",
       " ('ever', 160),\n",
       " ('seem', 159),\n",
       " ('discourse', 158),\n",
       " ('several', 158),\n",
       " ('dollar', 158),\n",
       " ('theory', 157),\n",
       " ('method', 157),\n",
       " ('why', 157),\n",
       " ('win', 157),\n",
       " ('step', 157),\n",
       " ('student', 156),\n",
       " ('simple', 156),\n",
       " ('put', 155),\n",
       " ('thousand', 155),\n",
       " ('next', 154),\n",
       " ('important', 154),\n",
       " ('friend', 154),\n",
       " ('ship', 154),\n",
       " ('linguistics', 153),\n",
       " ('rate', 153),\n",
       " ('guarantee', 153),\n",
       " ('legal', 152),\n",
       " ('fact', 150),\n",
       " ('note', 150),\n",
       " ('course', 150),\n",
       " ('floodgate', 150),\n",
       " ('under', 148),\n",
       " ('account', 147),\n",
       " ('talk', 147),\n",
       " ('return', 146),\n",
       " ('since', 146),\n",
       " ('allow', 146),\n",
       " ('school', 145),\n",
       " ('position', 145),\n",
       " ('american', 145),\n",
       " ('keep', 145),\n",
       " ('until', 145),\n",
       " ('result', 144),\n",
       " ('analysis', 143),\n",
       " ('experience', 143),\n",
       " ('present', 143),\n",
       " ('ad', 143),\n",
       " ('study', 142),\n",
       " ('reference', 142),\n",
       " ('remember', 142),\n",
       " ('another', 141),\n",
       " ('better', 141),\n",
       " ('sale', 141),\n",
       " ('speaker', 139),\n",
       " ('city', 139),\n",
       " ('wish', 139),\n",
       " ('problem', 139),\n",
       " ('group', 139),\n",
       " ('believe', 139),\n",
       " ('person', 139),\n",
       " ('bank', 138),\n",
       " ('full', 136),\n",
       " ('vium', 136),\n",
       " ('leave', 136),\n",
       " ('once', 136),\n",
       " ('lot', 135),\n",
       " ('answer', 135),\n",
       " ('print', 135),\n",
       " ('family', 135),\n",
       " ('section', 135),\n",
       " ('international', 134),\n",
       " ('area', 134),\n",
       " ('total', 134),\n",
       " ('complete', 134),\n",
       " ('plus', 134),\n",
       " ('georgetown', 134),\n",
       " ('department', 133),\n",
       " ('date', 133),\n",
       " ('grammar', 133),\n",
       " ('too', 133),\n",
       " ('second', 132),\n",
       " ('office', 132),\n",
       " ('lose', 132),\n",
       " ('bill', 131),\n",
       " ('issue', 131),\n",
       " ('future', 131),\n",
       " ('earn', 131),\n",
       " ('while', 130),\n",
       " ('above', 130),\n",
       " ('further', 129),\n",
       " ('long', 129),\n",
       " ('mean', 127),\n",
       " ('amount', 127),\n",
       " ('ca', 126),\n",
       " ('set', 126),\n",
       " ('join', 126),\n",
       " ('run', 125),\n",
       " ('purchase', 125),\n",
       " ('publish', 124),\n",
       " ('server', 124),\n",
       " ('possible', 123),\n",
       " ('become', 123),\n",
       " ('without', 123),\n",
       " ('nothing', 123),\n",
       " ('adult', 123),\n",
       " ('video', 122),\n",
       " ('still', 121),\n",
       " ('real', 121),\n",
       " ('something', 120),\n",
       " ('base', 120),\n",
       " ('accept', 120),\n",
       " ('already', 120),\n",
       " ('code', 119),\n",
       " ('registration', 118),\n",
       " ('uk', 118),\n",
       " ('sign', 118),\n",
       " ('everyone', 118),\n",
       " ('game', 118),\n",
       " ('support', 117),\n",
       " ('major', 117),\n",
       " ('personal', 117),\n",
       " ('understand', 116),\n",
       " ('fee', 116),\n",
       " ('however', 115),\n",
       " ('someone', 115),\n",
       " ('record', 114),\n",
       " ('speech', 114),\n",
       " ('small', 113),\n",
       " ('visit', 113),\n",
       " ('detail', 113),\n",
       " ('minute', 112),\n",
       " ('bonus', 112),\n",
       " ('mailing', 112),\n",
       " ('discussion', 111),\n",
       " ('able', 110),\n",
       " ('hear', 110),\n",
       " ('decide', 110),\n",
       " ('limit', 110),\n",
       " ('john', 109),\n",
       " ('human', 108),\n",
       " ('job', 108),\n",
       " ('hard', 107),\n",
       " ('guide', 107),\n",
       " ('child', 106),\n",
       " ('alway', 106),\n",
       " ('aol', 105),\n",
       " ('begin', 104),\n",
       " ('rule', 104),\n",
       " ('are', 104),\n",
       " ('tax', 104),\n",
       " ('online', 104),\n",
       " ('exactly', 104),\n",
       " ('additional', 103),\n",
       " ('reason', 103),\n",
       " ('past', 103),\n",
       " ('virtual', 103),\n",
       " ('secret', 103),\n",
       " ('either', 102),\n",
       " ('papers', 102),\n",
       " ('open', 102),\n",
       " ('payment', 101),\n",
       " ('general', 101),\n",
       " ('speak', 101),\n",
       " ('charge', 101),\n",
       " ('actually', 101),\n",
       " ('three', 100),\n",
       " ('verb', 100),\n",
       " ('ye', 100),\n",
       " ('customer', 100),\n",
       " ('teacher', 99),\n",
       " ('application', 99),\n",
       " ('content', 99),\n",
       " ('sound', 99),\n",
       " ('investment', 99),\n",
       " ('break', 98),\n",
       " ('technology', 98),\n",
       " ('access', 98),\n",
       " ('fill', 97),\n",
       " ('happen', 97),\n",
       " ('anywhere', 97),\n",
       " ('immediately', 97),\n",
       " ('profit', 97),\n",
       " ('usa', 96),\n",
       " ('structure', 96),\n",
       " ('washington', 96),\n",
       " ('pass', 96),\n",
       " ('choose', 96),\n",
       " ('ed', 95),\n",
       " ('though', 95),\n",
       " ('knowledge', 95),\n",
       " ('build', 94),\n",
       " ('electronic', 94),\n",
       " ('assume', 94),\n",
       " ('html', 94),\n",
       " ('true', 94),\n",
       " ('anything', 93),\n",
       " ('communication', 93),\n",
       " ('ac', 93),\n",
       " ('acquisition', 93),\n",
       " ('abstract', 93),\n",
       " ('sex', 93),\n",
       " ('feel', 93),\n",
       " ('direct', 92),\n",
       " ('wait', 92),\n",
       " ('consider', 91),\n",
       " ('meet', 91),\n",
       " ('topic', 91),\n",
       " ('material', 91),\n",
       " ('yourself', 91),\n",
       " ('contain', 90),\n",
       " ('hand', 90),\n",
       " ('syntax', 89),\n",
       " ('old', 89),\n",
       " ('datum', 89),\n",
       " ('manual', 89),\n",
       " ('end', 89),\n",
       " ('soon', 89),\n",
       " ('single', 88),\n",
       " ('title', 88),\n",
       " ('standard', 88),\n",
       " ('professional', 88),\n",
       " ('generate', 88),\n",
       " ('ago', 87),\n",
       " ('later', 87),\n",
       " ('claim', 87),\n",
       " ('bring', 87),\n",
       " ('science', 87),\n",
       " ('continue', 87),\n",
       " ('postal', 87),\n",
       " ('create', 87),\n",
       " ('national', 86),\n",
       " ('away', 86),\n",
       " ('envelope', 86),\n",
       " ('participant', 85),\n",
       " ('kind', 85),\n",
       " ('everything', 85),\n",
       " ('top', 84),\n",
       " ('delete', 84),\n",
       " ('matter', 83),\n",
       " ('else', 83),\n",
       " ('feature', 83),\n",
       " ('hundred', 83),\n",
       " ('links', 82),\n",
       " ('development', 81),\n",
       " ('phonology', 81),\n",
       " ('center', 81),\n",
       " ('march', 81),\n",
       " ('stop', 81),\n",
       " ('window', 81),\n",
       " ('orders', 81),\n",
       " ('natural', 80),\n",
       " ('telephone', 80),\n",
       " ('mention', 80),\n",
       " ('involve', 80),\n",
       " ('share', 80),\n",
       " ('express', 80),\n",
       " ('pp', 80),\n",
       " ('currency', 80),\n",
       " ('source', 79),\n",
       " ('session', 79),\n",
       " ('german', 79),\n",
       " ('apply', 79),\n",
       " ('stealth', 79),\n",
       " ('idea', 78),\n",
       " ('history', 78),\n",
       " ('version', 78),\n",
       " ('large', 78),\n",
       " ('far', 78),\n",
       " ('action', 78),\n",
       " ('test', 78),\n",
       " ('ready', 78),\n",
       " ('target', 78),\n",
       " ('yours', 78),\n",
       " ('relate', 77),\n",
       " ('approach', 77),\n",
       " ('advantage', 77),\n",
       " ('car', 77),\n",
       " ('country', 77),\n",
       " ('woman', 77),\n",
       " ('visa', 77),\n",
       " ('dream', 77),\n",
       " ('watch', 77),\n",
       " ('education', 76),\n",
       " ('view', 76),\n",
       " ('tool', 76),\n",
       " ('figure', 76),\n",
       " ('handle', 76),\n",
       " ('man', 75),\n",
       " ('effort', 75),\n",
       " ('risk', 75),\n",
       " ('construction', 74),\n",
       " ('spend', 74),\n",
       " ('chair', 74),\n",
       " ('potential', 74),\n",
       " ('russian', 74),\n",
       " ('chance', 74),\n",
       " ('value', 73),\n",
       " ('high', 73),\n",
       " ('easily', 73),\n",
       " ('hold', 73),\n",
       " ('zip', 73),\n",
       " ('whole', 73),\n",
       " ('debt', 73),\n",
       " ('reports', 73),\n",
       " ('dr', 72),\n",
       " ('house', 72),\n",
       " ('extra', 72),\n",
       " ('current', 72),\n",
       " ('organization', 72),\n",
       " ('turn', 72),\n",
       " ('press', 72),\n",
       " ('prove', 72),\n",
       " ('download', 72),\n",
       " ('offshore', 72),\n",
       " ('law', 71),\n",
       " ('mind', 71),\n",
       " ('reach', 71),\n",
       " ('capitalfm', 71),\n",
       " ('field', 70),\n",
       " ('original', 70),\n",
       " ('class', 70),\n",
       " ('age', 70),\n",
       " ('drive', 70),\n",
       " ('collect', 70),\n",
       " ('music', 70),\n",
       " ('fun', 70),\n",
       " ('always', 70),\n",
       " ('society', 69),\n",
       " ('review', 69),\n",
       " ('street', 69),\n",
       " ('publication', 68),\n",
       " ('discover', 68),\n",
       " ('almost', 68),\n",
       " ('cut', 68),\n",
       " ('enter', 68),\n",
       " ('story', 68),\n",
       " ('york', 68),\n",
       " ('security', 68),\n",
       " ('social', 67),\n",
       " ('around', 67),\n",
       " ('enclose', 67),\n",
       " ('dialect', 67),\n",
       " ('pronoun', 67),\n",
       " ('term', 67),\n",
       " ('effective', 67),\n",
       " ('against', 67),\n",
       " ('dear', 67),\n",
       " ('select', 67),\n",
       " ('format', 67),\n",
       " ('probably', 67),\n",
       " ('advertisement', 67),\n",
       " ('engine', 67),\n",
       " ('millions', 67),\n",
       " ('la', 66),\n",
       " ('successful', 66),\n",
       " ('power', 66),\n",
       " ('hope', 66),\n",
       " ('respond', 66),\n",
       " ('st', 66),\n",
       " ('ny', 66),\n",
       " ('means', 66),\n",
       " ('individual', 66),\n",
       " ('doubt', 66),\n",
       " ('room', 65),\n",
       " ('various', 65),\n",
       " ('vowel', 65),\n",
       " ('rest', 65),\n",
       " ('completely', 65),\n",
       " ('user', 65),\n",
       " ('directly', 64),\n",
       " ('whether', 64),\n",
       " ('yet', 64),\n",
       " ('along', 64),\n",
       " ('inc', 64),\n",
       " ('travel', 64),\n",
       " ('big', 64),\n",
       " ('foreign', 63),\n",
       " ('deal', 63),\n",
       " ('rather', 63),\n",
       " ('native', 63),\n",
       " ('sentence', 63),\n",
       " ('sales', 63),\n",
       " ('quite', 62),\n",
       " ('short', 62),\n",
       " ('register', 62),\n",
       " ('train', 62),\n",
       " ('institute', 62),\n",
       " ('finally', 62),\n",
       " ('appear', 62),\n",
       " ('goal', 62),\n",
       " ('comment', 62),\n",
       " ('together', 62),\n",
       " ('connection', 62),\n",
       " ('mass', 62),\n",
       " ('increase', 62),\n",
       " ('shor', 62),\n",
       " ('david', 61),\n",
       " ('phonetic', 61),\n",
       " ('produce', 61),\n",
       " ('enough', 61),\n",
       " ('id', 61),\n",
       " ('object', 60),\n",
       " ('invite', 60),\n",
       " ('vol', 60),\n",
       " ('raise', 60),\n",
       " ('certain', 60),\n",
       " ('resource', 60),\n",
       " ('miss', 60),\n",
       " ('payable', 59),\n",
       " ('design', 59),\n",
       " ('expect', 59),\n",
       " ('summary', 59),\n",
       " ('main', 59),\n",
       " ('rich', 59),\n",
       " ('freedom', 59),\n",
       " ('huge', 59),\n",
       " ('circular', 58),\n",
       " ('french', 58),\n",
       " ('phenomenon', 58),\n",
       " ('chinese', 58),\n",
       " ('develop', 58),\n",
       " ('quality', 58),\n",
       " ('japanese', 58),\n",
       " ('capital', 58),\n",
       " ('website', 58),\n",
       " ('query', 57),\n",
       " ('attention', 57),\n",
       " ('ability', 57),\n",
       " ('semantic', 57),\n",
       " ('journal', 57),\n",
       " ('perhap', 57),\n",
       " ('america', 57),\n",
       " ('play', 57),\n",
       " ('explain', 57),\n",
       " ('entire', 57),\n",
       " ('mastercard', 57),\n",
       " ('control', 56),\n",
       " ('culture', 56),\n",
       " ('advance', 56),\n",
       " ('domain', 56),\n",
       " ('academic', 56),\n",
       " ('index', 56),\n",
       " ('lead', 56),\n",
       " ('multus', 56),\n",
       " ('happy', 56),\n",
       " ('literature', 55),\n",
       " ('article', 55),\n",
       " ('aspect', 55),\n",
       " ('due', 55),\n",
       " ('po', 55),\n",
       " ('historical', 55),\n",
       " ('model', 55),\n",
       " ('local', 55),\n",
       " ('federal', 55),\n",
       " ('dept', 55),\n",
       " ('low', 55),\n",
       " ('info', 55),\n",
       " ('especially', 55),\n",
       " ('difference', 55),\n",
       " ('associate', 55),\n",
       " ('don', 55),\n",
       " ('reduplication', 54),\n",
       " ('exchange', 54),\n",
       " ('specific', 54),\n",
       " ('mary', 54),\n",
       " ('digital', 54),\n",
       " ('mark', 54),\n",
       " ('six', 54),\n",
       " ('master', 54),\n",
       " ('public', 54),\n",
       " ('loan', 54),\n",
       " ('sexual', 54),\n",
       " ('tm', 54),\n",
       " ('cover', 53),\n",
       " ('correct', 53),\n",
       " ('computational', 53),\n",
       " ('author', 53),\n",
       " ('common', 53),\n",
       " ('suite', 53),\n",
       " ('latest', 53),\n",
       " ('close', 53),\n",
       " ('piece', 53),\n",
       " ('recruit', 53),\n",
       " ('filter', 53),\n",
       " ('european', 52),\n",
       " ('recent', 52),\n",
       " ('spanish', 52),\n",
       " ('project', 52),\n",
       " ('track', 52),\n",
       " ('judgment', 52),\n",
       " ('exist', 52),\n",
       " ('mailer', 52),\n",
       " ('kid', 52),\n",
       " ('seven', 52),\n",
       " ('fresh', 52),\n",
       " ('profitable', 52),\n",
       " ('hotel', 51),\n",
       " ('prepare', 51),\n",
       " ('le', 51),\n",
       " ('volume', 51),\n",
       " ('college', 51),\n",
       " ('often', 51),\n",
       " ('absolutely', 51),\n",
       " ('june', 51),\n",
       " ('january', 51),\n",
       " ('sometime', 51),\n",
       " ('concern', 51),\n",
       " ('deliver', 51),\n",
       " ('error', 51),\n",
       " ('news', 51),\n",
       " ('provider', 51),\n",
       " ('invest', 51),\n",
       " ('deat', 50),\n",
       " ('submit', 50),\n",
       " ('announcement', 50),\n",
       " ('formal', 50),\n",
       " ('locate', 50),\n",
       " ('love', 50),\n",
       " ('direction', 50),\n",
       " ('themselve', 50),\n",
       " ('government', 50),\n",
       " ('discuss', 49),\n",
       " ('couple', 49),\n",
       " ('population', 49),\n",
       " ('grammatical', 49),\n",
       " ('indicate', 49),\n",
       " ('committee', 49),\n",
       " ('speed', 49),\n",
       " ('cognitive', 49),\n",
       " ('century', 49),\n",
       " ('qualify', 49),\n",
       " ('enjoy', 49),\n",
       " ('database', 49),\n",
       " ('compuserve', 49),\n",
       " ('hit', 49),\n",
       " ('hot', 49),\n",
       " ('mlm', 49),\n",
       " ('pragmatic', 48),\n",
       " ('third', 48),\n",
       " ('particle', 48),\n",
       " ('upon', 48),\n",
       " ('half', 48),\n",
       " ('grow', 48),\n",
       " ('category', 48),\n",
       " ('situation', 48),\n",
       " ('californium', 48),\n",
       " ('update', 48),\n",
       " ('technical', 48),\n",
       " ('worldwide', 48),\n",
       " ('refer', 48),\n",
       " ('intelligence', 48),\n",
       " ('imagine', 48),\n",
       " ('club', 48),\n",
       " ('powerful', 48),\n",
       " ('fast', 48),\n",
       " ('goldrush', 48),\n",
       " ('dori', 48),\n",
       " ('context', 47),\n",
       " ('wide', 47),\n",
       " ('modern', 47),\n",
       " ('suggest', 47),\n",
       " ('myself', 47),\n",
       " ('agree', 47),\n",
       " ('distinction', 47),\n",
       " ('morphology', 47),\n",
       " ('hr', 47),\n",
       " ('sort', 47),\n",
       " ('discount', 47),\n",
       " ('htm', 47),\n",
       " ('tel', 46),\n",
       " ('syntactic', 46),\n",
       " ('recently', 46),\n",
       " ('clear', 46),\n",
       " ('longer', 46),\n",
       " ('submission', 46),\n",
       " ('cannot', 46),\n",
       " ('phrase', 46),\n",
       " ('length', 46),\n",
       " ('pick', 46),\n",
       " ('girl', 46),\n",
       " ('excite', 46),\n",
       " ('isp', 46),\n",
       " ('signature', 45),\n",
       " ('wonder', 45),\n",
       " ('unite', 45),\n",
       " ('respondent', 45),\n",
       " ('principle', 45),\n",
       " ('particular', 45),\n",
       " ('basis', 45),\n",
       " ('similar', 45),\n",
       " ('early', 45),\n",
       " ('toll', 45),\n",
       " ('benefit', 45),\n",
       " ('sincerely', 45),\n",
       " ('works', 45),\n",
       " ('boyfriend', 45),\n",
       " ('lunch', 44),\n",
       " ('relative', 44),\n",
       " ('forward', 44),\n",
       " ('instead', 44),\n",
       " ('july', 44),\n",
       " ('sheet', 44),\n",
       " ('road', 44),\n",
       " ('space', 44),\n",
       " ('period', 44),\n",
       " ('delivery', 44),\n",
       " ('yes', 44),\n",
       " ('quickly', 44),\n",
       " ('using', 44),\n",
       " ('tense', 44),\n",
       " ('partner', 44),\n",
       " ('five', 44),\n",
       " ('perfectly', 44),\n",
       " ('ems', 44),\n",
       " ('february', 43),\n",
       " ('propose', 43),\n",
       " ('candidate', 43),\n",
       " ('sample', 43),\n",
       " ('non', 43),\n",
       " ('lexical', 43),\n",
       " ('outside', 43),\n",
       " ('evidence', 43),\n",
       " ('initial', 43),\n",
       " ('quick', 43),\n",
       " ('membership', 43),\n",
       " ('worth', 43),\n",
       " ('private', 43),\n",
       " ('movement', 43),\n",
       " ('luck', 43),\n",
       " ('postage', 43),\n",
       " ('der', 43),\n",
       " ('reduce', 43),\n",
       " ('chechen', 43),\n",
       " ('forget', 42),\n",
       " ('practice', 42),\n",
       " ('commercial', 42),\n",
       " ('deaf', 42),\n",
       " ('canada', 42),\n",
       " ('legitimate', 42),\n",
       " ('comparison', 42),\n",
       " ('workshop', 42),\n",
       " ('industry', 42),\n",
       " ('beautiful', 42),\n",
       " ('chain', 42),\n",
       " ('campaign', 42),\n",
       " ('cent', 42),\n",
       " ('duplicate', 42),\n",
       " ('celebrity', 42),\n",
       " ('effect', 41),\n",
       " ('inquiry', 41),\n",
       " ('actual', 41),\n",
       " ('community', 41),\n",
       " ('paul', 41),\n",
       " ('frank', 41),\n",
       " ('shop', 41),\n",
       " ('technique', 41),\n",
       " ('picture', 41),\n",
       " ('introduce', 41),\n",
       " ('phonological', 41),\n",
       " ('forum', 41),\n",
       " ('enterprise', 41),\n",
       " ('regard', 41),\n",
       " ('act', 41),\n",
       " ('ok', 41),\n",
       " ('truly', 41),\n",
       " ('agency', 41),\n",
       " ('rights', 41),\n",
       " ('paradise', 41),\n",
       " ('alter', 41),\n",
       " ('self', 41),\n",
       " ('corporation', 41),\n",
       " ('amaze', 41),\n",
       " ('unique', 40),\n",
       " ('independent', 40),\n",
       " ('quote', 40),\n",
       " ('evaluation', 40),\n",
       " ('drop', 40),\n",
       " ('theoretical', 40),\n",
       " ('replace', 40),\n",
       " ('universal', 40),\n",
       " ('presentation', 40),\n",
       " ('voice', 40),\n",
       " ('table', 40),\n",
       " ('global', 40),\n",
       " ('document', 40),\n",
       " ('thanks', 40),\n",
       " ('compare', 40),\n",
       " ('stock', 40),\n",
       " ('collection', 40),\n",
       " ('alone', 40),\n",
       " ('org', 40),\n",
       " ('monthly', 40),\n",
       " ('optional', 40),\n",
       " ('marketing', 40),\n",
       " ('september', 39),\n",
       " ('activity', 39),\n",
       " ('night', 39),\n",
       " ('prefer', 39),\n",
       " ('organizer', 39),\n",
       " ('au', 39),\n",
       " ('although', 39),\n",
       " ('disk', 39),\n",
       " ('richard', 39),\n",
       " ('higher', 39),\n",
       " ('achieve', 39),\n",
       " ('treat', 39),\n",
       " ('fund', 39),\n",
       " ('description', 39),\n",
       " ('nor', 39),\n",
       " ('artificial', 39),\n",
       " ('operate', 39),\n",
       " ('choice', 39),\n",
       " ('automatically', 39),\n",
       " ('hello', 39),\n",
       " ('girlfriend', 39),\n",
       " ('north', 38),\n",
       " ('depend', 38),\n",
       " ('maybe', 38),\n",
       " ('tape', 38),\n",
       " ('seminar', 38),\n",
       " ('introduction', 38),\n",
       " ('block', 38),\n",
       " ('beach', 38),\n",
       " ('graphic', 38),\n",
       " ('url', 38),\n",
       " ('except', 38),\n",
       " ('expression', 38),\n",
       " ('promise', 38),\n",
       " ('clean', 38),\n",
       " ('moment', 38),\n",
       " ('retire', 38),\n",
       " ('spam', 38),\n",
       " ('late', 37),\n",
       " ('degree', 37),\n",
       " ('final', 37),\n",
       " ('graduate', 37),\n",
       " ('focus', 37),\n",
       " ('certainly', 37),\n",
       " ('across', 37),\n",
       " ('conversation', 37),\n",
       " ('argument', 37),\n",
       " ('dc', 37),\n",
       " ('hesitate', 37),\n",
       " ('task', 37),\n",
       " ('concept', 37),\n",
       " ('necessary', 37),\n",
       " ('daily', 37),\n",
       " ('co', 37),\n",
       " ('sake', 37),\n",
       " ('insurance', 37),\n",
       " ('proof', 37),\n",
       " ('totally', 37),\n",
       " ('benjamin', 37),\n",
       " ('forever', 37),\n",
       " ('rom', 37),\n",
       " ('color', 37),\n",
       " ('mortgage', 37),\n",
       " ('relax', 37),\n",
       " ('michael', 36),\n",
       " ('sense', 36),\n",
       " ('relationship', 36),\n",
       " ('highly', 36),\n",
       " ('whatever', 36),\n",
       " ('addition', 36),\n",
       " ('deadline', 36),\n",
       " ('south', 36),\n",
       " ('eliminate', 36),\n",
       " ('cc', 36),\n",
       " ('describe', 36),\n",
       " ('symbol', 36),\n",
       " ('guvax', 36),\n",
       " ('interpretation', 36),\n",
       " ('background', 36),\n",
       " ('obviously', 36),\n",
       " ('succeed', 36),\n",
       " ('editor', 36),\n",
       " ('clause', 36),\n",
       " ('pc', 36),\n",
       " ('air', 36),\n",
       " ('header', 36),\n",
       " ('extremely', 36),\n",
       " ('prompt', 36),\n",
       " ('ingush', 36),\n",
       " ('trade', 36),\n",
       " ('competition', 36),\n",
       " ('client', 36),\n",
       " ('addresses', 36),\n",
       " ('resell', 36),\n",
       " ('refund', 36),\n",
       " ('semantics', 35),\n",
       " ('item', 35),\n",
       " ('transfer', 35),\n",
       " ('participation', 35),\n",
       " ('obtain', 35),\n",
       " ('edinburgh', 35),\n",
       " ('inform', 35),\n",
       " ('ten', 35),\n",
       " ('among', 35),\n",
       " ('london', 35),\n",
       " ('synthetic', 35),\n",
       " ('element', 35),\n",
       " ('december', 35),\n",
       " ('separate', 35),\n",
       " ('bottom', 35),\n",
       " ('toward', 35),\n",
       " ('germany', 35),\n",
       " ('regular', 35),\n",
       " ('classify', 35),\n",
       " ('stem', 35),\n",
       " ('ipa', 35),\n",
       " ('multiple', 35),\n",
       " ('release', 35),\n",
       " ('accurately', 35),\n",
       " ('undeliverable', 35),\n",
       " ('cultural', 34),\n",
       " ('currently', 34),\n",
       " ('centre', 34),\n",
       " ('skill', 34),\n",
       " ('suggestion', 34),\n",
       " ('range', 34),\n",
       " ...]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To show the most frequent words in train-mails\n",
    "dictionary = make_Dictionary('ling-spam/train-mails')\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59cfe5b-2a63-43b0-b0de-85069b89e280",
   "metadata": {},
   "source": [
    "### Feature Extraction Process\n",
    "Once the dictionary is ready, we can extract word count vector (our feature here) of 3000 dimensions for each email of the training set. Each **word count vector** contains the frequency of 3000 words in the training file. Of course you might have guessed by now that most of them will be zero. Let us take an example. Suppose we have 500 words in our dictionary. Each word count vector contains the frequency of 500 dictionary words in the training file. Suppose the text in the training file is “Get the work done, work done”, then it will be encoded as $$[0,0,0,0,0,…….0,0,2,0,0,0,……,0,0,1,0,0,…0,0,1,0,0,……2,0,0,0,0,0]$$ Here, all the word counts are placed at the 296th, 359th, 415th, 495th elements of the word count vector in the length of 500 and the rest are zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e183714-de20-497b-ac13-975ba7aff07d",
   "metadata": {},
   "source": [
    "The below python code will generate a feature vector matrix whose rows denote 700 files of the training set and columns denote 3000 words of the dictionary. The value at index ${ij}$ will be the number of occurrences of the $j^{th}$ word of the dictionary in the $i^{th}$ file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5a60efca-9dda-4839-8c89-8dda3756c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(mail_dir):\n",
    "    files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
    "    features_matrix = np.zeros((len(files),3000))\n",
    "    label = np.zeros(len(files))\n",
    "    docID = 0\n",
    "    _i = 0\n",
    "    print(len(files))\n",
    "    for fil in files:\n",
    "        _i+=1\n",
    "        label[docID] = int('spmsg' in fil)\n",
    "        with open(fil) as fi:\n",
    "            for i,line in enumerate(fi):\n",
    "                if i == 2:\n",
    "                    words = line.split()\n",
    "                    for word in words:\n",
    "                        wordID = 0\n",
    "                        for i,d in enumerate(dictionary):\n",
    "                            if d[0] == word:\n",
    "                                wordID = i\n",
    "                                features_matrix[docID,wordID]+=1\n",
    "            docID = docID + 1\n",
    "        print('\\r','done {} files'.format(_i),flush=True,end='')\n",
    "    return features_matrix, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec33d1e-a61a-46c0-adab-487d3cea2415",
   "metadata": {},
   "source": [
    "### Training the Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4fd4e379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702\n",
      " done 702 files260\n",
      " done 260 files"
     ]
    }
   ],
   "source": [
    "x_train, y_train = extract_features(\"ling-spam/train-mails\")\n",
    "x_test, y_test = extract_features(\"ling-spam/test-mails\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d9f437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNB:\n",
    "    def __init__(self):\n",
    "        self.classes = None\n",
    "        self.mean = {}\n",
    "        self.var = {}\n",
    "        self.prior = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        for _, c in enumerate(self.classes):\n",
    "            label = \"class\" + str(c)\n",
    "            X_Index_c = X[np.where(y == c)]\n",
    "            self.mean[label] = np.mean(X_Index_c, axis=0, keepdims=True)\n",
    "            self.var[label] = np.var(X_Index_c, axis=0, keepdims=True)\n",
    "            self.prior[label] = X_Index_c.shape[0] / X.shape[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        # return class with the highest probability\n",
    "        prediction = []\n",
    "        for cur_X in X:\n",
    "            cur_result = {}\n",
    "            for c in self.classes:\n",
    "                label = \"class\" + str(c)\n",
    "                mean = self.mean[label][0]\n",
    "                var = self.var[label][0]\n",
    "                prior = np.log(self.prior[label])\n",
    "                for x, m, v in zip(cur_X, mean, var):\n",
    "                    if v:\n",
    "                        x_pro = (\n",
    "                            1.0\n",
    "                            / np.sqrt(2.0 * np.pi * v)\n",
    "                            * np.exp(-np.square(x - m) / (2.0 * v))\n",
    "                        )\n",
    "                    else:\n",
    "                        x_pro = int(m == x)\n",
    "                    prior += np.log(x_pro + 1e-200)\n",
    "                cur_result[c] = prior\n",
    "            prediction.append(max(cur_result, key=cur_result.get))\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfe78ea-e607-4f3c-8b44-fe0fdd1187cc",
   "metadata": {},
   "source": [
    "### Checking Performance\n",
    "The test set contains 130 spam emails and 130 non-spam emails. Please compute accuracy, recall, F-1 score to evaluate the performance of your spam filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "adffeaf5-1841-4207-8100-a1ef628b28f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Accuracy: 0.9730769230769231\n",
      "Custom Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.97       130\n",
      "         1.0       1.00      0.95      0.97       130\n",
      "\n",
      "    accuracy                           0.97       260\n",
      "   macro avg       0.97      0.97      0.97       260\n",
      "weighted avg       0.97      0.97      0.97       260\n",
      "\n",
      "Sklearn Accuracy: 0.9615384615384616\n",
      "Sklearn Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.99      0.96       130\n",
      "         1.0       0.99      0.93      0.96       130\n",
      "\n",
      "    accuracy                           0.96       260\n",
      "   macro avg       0.96      0.96      0.96       260\n",
      "weighted avg       0.96      0.96      0.96       260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Custom Accuracy:\", accuracy)\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Custom Classification Report:\\n\", report)\n",
    "############################################\n",
    "from sklearn.naive_bayes import GaussianNB as SKGaussianNB\n",
    "model = SKGaussianNB()\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Sklearn Accuracy:\", accuracy)\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Sklearn Classification Report:\\n\", report)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dccf609-0f9c-439b-b9ca-62e78796a7a7",
   "metadata": {},
   "source": [
    "### Questions\n",
    "1. Describe another real-world application where the naïve Bayes method can be applied\n",
    "2. What are the strengths of the naïve Bayes method; when does it perform well?\n",
    "3. What are the weaknesses of the naïve Bayes method; when does it perform poorly?\n",
    "4. What makes the naïve Bayes method a good candidate for the classification problem, if you have enough knowledge about the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dfcb7c",
   "metadata": {},
   "source": [
    "1. **Real-World Application of Naive Bayes:**\n",
    "   Naive Bayes is a versatile algorithm used in various real-world applications. One such application is **sentiment analysis** in natural language processing. \n",
    "\n",
    "2. **Strengths of Naive Bayes:**\n",
    "   - **Simplicity:** Naive Bayes is a straightforward and easy-to-understand algorithm, making it suitable for both beginners and experts.\n",
    "   - **Efficiency:** It is computationally efficient and works well with high-dimensional data.\n",
    "   - **Good for Text Data:** Naive Bayes often performs well on text classification tasks, such as spam detection and sentiment analysis.\n",
    "   - **Handles Missing Data:** It can handle missing values gracefully by simply ignoring them.\n",
    "   - **Scalability:** Naive Bayes is highly scalable, making it suitable for large datasets.\n",
    "\n",
    "   It performs well when:\n",
    "   - Features are conditionally independent (the \"naive\" assumption).\n",
    "   - The dataset has categorical or discrete features.\n",
    "   - There are many features relative to the size of the dataset.\n",
    "\n",
    "3. **Weaknesses of Naive Bayes:**\n",
    "   - **Simplistic Assumption:** The primary weakness is the assumption of feature independence, which may not hold true in many real-world datasets.\n",
    "   - **Limited Expressiveness:** Naive Bayes may not capture complex relationships between features.\n",
    "   - **Sensitive to Data Quality:** It can be sensitive to the quality of training data, especially when dealing with imbalanced datasets.\n",
    "   - **Doesn't Handle Continuous Data Well:** It's not the best choice for datasets with continuous features unless discretization is applied.\n",
    "\n",
    "   It performs poorly when:\n",
    "   - Features are not conditionally independent (violation of the naive assumption).\n",
    "   - The dataset contains a significant amount of continuous or real-valued data.\n",
    "   - The data quality is poor, or there's a class imbalance issue.\n",
    "\n",
    "4. **Why Naive Bayes for Classification:**\n",
    "   Naive Bayes can be a good candidate for classification problems, especially when you have enough knowledge about the data, because:\n",
    "   - It's computationally efficient and scales well, even with large datasets.\n",
    "   - It's easy to implement and interpret, making it a quick choice for prototyping and initial analysis.\n",
    "   - It often performs surprisingly well on text data, making it suitable for tasks like spam detection, sentiment analysis, and document classification.\n",
    "   - It's a useful baseline model for text classification tasks, providing a simple benchmark against which more complex models can be compared."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1eb71e714c2a1bbfc91d3a1ed02399b846367c62fca82a5b3a9325d7b60709de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
